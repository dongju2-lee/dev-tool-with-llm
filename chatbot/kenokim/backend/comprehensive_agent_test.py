"""
ì¢…í•©ì ì¸ Agent í…ŒìŠ¤íŠ¸ ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  LLMì„ í†µí•´ ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass

from langchain_google_genai import ChatGoogleGenerativeAI
from app.graph.instance import process_chat_message
from app.core.config import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TestScenario:
    """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜"""
    name: str
    description: str
    user_input: str
    expected_agent: str
    expected_behavior: str
    success_criteria: str
    category: str

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    scenario: TestScenario
    agent_response: str
    agent_used: str
    tools_used: List[str]
    response_length: int
    execution_time: float
    llm_evaluation: Dict[str, Any]
    success: bool
    error: str = None

class AgentEvaluator:
    """LLMì„ ì´ìš©í•œ Agent ì‘ë‹µ í‰ê°€"""
    
    def __init__(self):
        self.evaluator_llm = ChatGoogleGenerativeAI(
            model=settings.gemini_model,
            google_api_key=settings.gemini_api_key,
            temperature=0.1
        )
    
    async def evaluate_response(self, scenario: TestScenario, response: str, agent_used: str, tools_used: List[str]) -> Dict[str, Any]:
        """LLMì„ í†µí•´ ì‘ë‹µì„ í‰ê°€í•©ë‹ˆë‹¤."""
        
        evaluation_prompt = f"""
ë‹¤ìŒ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ AI Agentì˜ ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”.

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:**
- ì¹´í…Œê³ ë¦¬: {scenario.category}
- ì„¤ëª…: {scenario.description}
- ì‚¬ìš©ì ì…ë ¥: "{scenario.user_input}"
- ê¸°ëŒ€ ì—ì´ì „íŠ¸: {scenario.expected_agent}
- ê¸°ëŒ€ ë™ì‘: {scenario.expected_behavior}
- ì„±ê³µ ê¸°ì¤€: {scenario.success_criteria}

**Agent ì‘ë‹µ ê²°ê³¼:**
- ì‚¬ìš©ëœ ì—ì´ì „íŠ¸: {agent_used}
- ì‚¬ìš©ëœ ë„êµ¬: {tools_used}
- ì‘ë‹µ ë‚´ìš©: "{response}"
- ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì

**í‰ê°€ ê¸°ì¤€:**
1. **ì—ì´ì „íŠ¸ ì„ íƒ ì ì ˆì„±** (1-5ì ): ì˜¬ë°”ë¥¸ ì „ë¬¸ ì—ì´ì „íŠ¸ê°€ ì„ íƒë˜ì—ˆëŠ”ê°€?
2. **ìš”ì²­ ì´í•´ë„** (1-5ì ): ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í–ˆëŠ”ê°€?
3. **ì‘ë‹µ ì™„ì„±ë„** (1-5ì ): ìš”ì²­ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí–ˆëŠ”ê°€?
4. **ì‘ë‹µ í’ˆì§ˆ** (1-5ì ): ì‘ë‹µì´ ëª…í™•í•˜ê³  ìœ ìš©í•œê°€?
5. **ì „ë°˜ì  ë§Œì¡±ë„** (1-5ì ): ì „ì²´ì ìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‘ë‹µì¸ê°€?

ë‹¤ìŒ JSON í˜•íƒœë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
{{
    "agent_selection_score": <1-5>,
    "understanding_score": <1-5>,
    "completeness_score": <1-5>,
    "quality_score": <1-5>,
    "overall_score": <1-5>,
    "total_score": <í•©ê³„>,
    "max_score": 25,
    "percentage": <ë°±ë¶„ìœ¨>,
    "success": <true/false>,
    "strengths": ["ê°•ì 1", "ê°•ì 2"],
    "weaknesses": ["ì•½ì 1", "ì•½ì 2"],
    "recommendations": ["ê°œì„ ì‚¬í•­1", "ê°œì„ ì‚¬í•­2"],
    "summary": "í•œ ì¤„ í‰ê°€ ìš”ì•½"
}}
"""

        try:
            response_obj = await self.evaluator_llm.ainvoke(evaluation_prompt)
            evaluation_text = response_obj.content
            
            # JSON ì¶”ì¶œ ì‹œë„
            start_idx = evaluation_text.find('{')
            end_idx = evaluation_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = evaluation_text[start_idx:end_idx]
                evaluation = json.loads(json_str)
            else:
                # JSON ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‰ê°€
                evaluation = {
                    "agent_selection_score": 3,
                    "understanding_score": 3,
                    "completeness_score": 3,
                    "quality_score": 3,
                    "overall_score": 3,
                    "total_score": 15,
                    "max_score": 25,
                    "percentage": 60,
                    "success": False,
                    "strengths": ["ì‘ë‹µ ìƒì„±ë¨"],
                    "weaknesses": ["í‰ê°€ ì‹¤íŒ¨"],
                    "recommendations": ["í‰ê°€ ì‹œìŠ¤í…œ ê°œì„  í•„ìš”"],
                    "summary": "í‰ê°€ ì²˜ë¦¬ ì‹¤íŒ¨"
                }
            
            return evaluation
            
        except Exception as e:
            logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "agent_selection_score": 1,
                "understanding_score": 1,
                "completeness_score": 1,
                "quality_score": 1,
                "overall_score": 1,
                "total_score": 5,
                "max_score": 25,
                "percentage": 20,
                "success": False,
                "strengths": [],
                "weaknesses": ["í‰ê°€ ì˜¤ë¥˜"],
                "recommendations": ["ì‹œìŠ¤í…œ ì ê²€ í•„ìš”"],
                "summary": f"í‰ê°€ ì˜¤ë¥˜: {str(e)}"
            }

class ComprehensiveAgentTester:
    """ì¢…í•©ì ì¸ Agent í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.evaluator = AgentEvaluator()
        self.test_scenarios = self._define_test_scenarios()
        self.results: List[TestResult] = []
    
    def _define_test_scenarios(self) -> List[TestScenario]:
        """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."""
        
        scenarios = [
            # ëŒ€ì‹œë³´ë“œ ëª©ë¡ ê´€ë ¨
            TestScenario(
                name="ëŒ€ì‹œë³´ë“œ_ëª©ë¡_ìš”ì²­",
                description="ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì‹œë³´ë“œ ëª©ë¡ì„ ìš”ì²­í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤",
                user_input="ëŒ€ì‹œë³´ë“œ ëª©ë¡ì„ ë³´ì—¬ì£¼ì„¸ìš”",
                expected_agent="grafana_renderer_mcp_agent",
                expected_behavior="ëŒ€ì‹œë³´ë“œ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ì‚¬ìš©ìì—ê²Œ ì œê³µ",
                success_criteria="ì‹¤ì œ ëŒ€ì‹œë³´ë“œ ëª©ë¡ì´ í¬í•¨ëœ ì‘ë‹µ",
                category="dashboard_list"
            ),
            
            TestScenario(
                name="ì‚¬ìš©ê°€ëŠ¥í•œ_ëŒ€ì‹œë³´ë“œ_ë¬¸ì˜",
                description="ì–´ë–¤ ëŒ€ì‹œë³´ë“œê°€ ìˆëŠ”ì§€ ìì—°ì–´ë¡œ ë¬¸ì˜",
                user_input="ì–´ë–¤ ëŒ€ì‹œë³´ë“œë“¤ì„ ë³¼ ìˆ˜ ìˆë‚˜ìš”?",
                expected_agent="grafana_renderer_mcp_agent",
                expected_behavior="ëŒ€ì‹œë³´ë“œ ëª©ë¡ ì¡°íšŒ ë° ì•ˆë‚´",
                success_criteria="ëŒ€ì‹œë³´ë“œ ì´ë¦„ë“¤ì´ ë‚˜ì—´ëœ ì‘ë‹µ",
                category="dashboard_list"
            ),
            
            # ëŒ€ì‹œë³´ë“œ ë Œë”ë§ ê´€ë ¨
            TestScenario(
                name="íŠ¹ì •_ëŒ€ì‹œë³´ë“œ_ë Œë”ë§",
                description="Node Exporter Full ëŒ€ì‹œë³´ë“œ ë Œë”ë§ ìš”ì²­",
                user_input="Node Exporter Full ëŒ€ì‹œë³´ë“œë¥¼ ë Œë”ë§í•´ì£¼ì„¸ìš”",
                expected_agent="grafana_renderer_mcp_agent",
                expected_behavior="ëŒ€ì‹œë³´ë“œë¥¼ ë Œë”ë§í•˜ì—¬ ì´ë¯¸ì§€ ì œê³µ",
                success_criteria="ë Œë”ë§ëœ ì´ë¯¸ì§€ ë°ì´í„° ë˜ëŠ” ì„±ê³µ ë©”ì‹œì§€",
                category="dashboard_render"
            ),
            
            TestScenario(
                name="ëŒ€ì‹œë³´ë“œ_ì‹œê°í™”_ìš”ì²­",
                description="ëŒ€ì‹œë³´ë“œë¥¼ ë³´ì—¬ë‹¬ë¼ëŠ” ìì—°ì–´ ìš”ì²­",
                user_input="Prometheus Stats ëŒ€ì‹œë³´ë“œë¥¼ ë³´ì—¬ì¤˜",
                expected_agent="grafana_renderer_mcp_agent",
                expected_behavior="í•´ë‹¹ ëŒ€ì‹œë³´ë“œ ë Œë”ë§",
                success_criteria="ëŒ€ì‹œë³´ë“œ ë Œë”ë§ ì‹œë„ ë° ê²°ê³¼ ì œê³µ",
                category="dashboard_render"
            ),
            
            # ë°ì´í„° ë¶„ì„ ê´€ë ¨
            TestScenario(
                name="ì‹œìŠ¤í…œ_ì„±ëŠ¥_ë¶„ì„",
                description="ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ ìš”ì²­",
                user_input="ì„œë²„ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                expected_agent="grafana_agent",
                expected_behavior="ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„ ìˆ˜í–‰",
                success_criteria="ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ë˜ëŠ” ë¶„ì„ ê³¼ì • ì„¤ëª…",
                category="data_analysis"
            ),
            
            TestScenario(
                name="ë©”ëª¨ë¦¬_ì‚¬ìš©ëŸ‰_í™•ì¸",
                description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ìš”ì²­",
                user_input="í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•´ì£¼ì„¸ìš”",
                expected_agent="grafana_agent",
                expected_behavior="ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ì¡°íšŒ ë° ë¶„ì„",
                success_criteria="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ì œê³µ",
                category="data_analysis"
            ),
            
            # ì¼ë°˜ì ì¸ ìƒí˜¸ì‘ìš©
            TestScenario(
                name="ì¸ì‚¬_ë°_ì†Œê°œ",
                description="ê¸°ë³¸ì ì¸ ì¸ì‚¬ ë° ì‹œìŠ¤í…œ ì†Œê°œ ìš”ì²­",
                user_input="ì•ˆë…•í•˜ì„¸ìš”, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆë‚˜ìš”?",
                expected_agent="supervisor",
                expected_behavior="ì¹œê·¼í•œ ì¸ì‚¬ ë° ê¸°ëŠ¥ ì†Œê°œ",
                success_criteria="ì ì ˆí•œ ì¸ì‚¬ë§ê³¼ ê¸°ëŠ¥ ì•ˆë‚´",
                category="general"
            ),
            
            TestScenario(
                name="ë„ì›€ë§_ìš”ì²­",
                description="ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ì— ëŒ€í•œ ë„ì›€ë§ ìš”ì²­",
                user_input="ì–´ë–¤ ê¸°ëŠ¥ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                expected_agent="supervisor",
                expected_behavior="ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤ ì•ˆë‚´",
                success_criteria="ê¸°ëŠ¥ ëª©ë¡ ë° ì‚¬ìš©ë²• ì•ˆë‚´",
                category="general"
            ),
            
            # ì• ë§¤í•œ ìš”ì²­
            TestScenario(
                name="ëª¨í˜¸í•œ_ê·¸ë¼íŒŒë‚˜_ìš”ì²­",
                description="êµ¬ì²´ì ì´ì§€ ì•Šì€ Grafana ê´€ë ¨ ìš”ì²­",
                user_input="ê·¸ë¼íŒŒë‚˜ ê´€ë ¨í•´ì„œ ë­”ê°€ í•´ì£¼ì„¸ìš”",
                expected_agent="grafana_renderer_mcp_agent",
                expected_behavior="êµ¬ì²´ì ì¸ ìš”ì²­ ì‚¬í•­ ë¬¸ì˜ ë˜ëŠ” ê¸°ë³¸ ë™ì‘ ìˆ˜í–‰",
                success_criteria="ì ì ˆí•œ ì•ˆë‚´ ë˜ëŠ” ê¸°ë³¸ ê¸°ëŠ¥ ì œê³µ",
                category="ambiguous"
            ),
            
            # ì˜¤ë¥˜ ìƒí™©
            TestScenario(
                name="ì¡´ì¬í•˜ì§€_ì•ŠëŠ”_ëŒ€ì‹œë³´ë“œ",
                description="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëŒ€ì‹œë³´ë“œ ìš”ì²­",
                user_input="NonExistent Dashboardë¥¼ ë Œë”ë§í•´ì£¼ì„¸ìš”",
                expected_agent="grafana_renderer_mcp_agent",
                expected_behavior="ëŒ€ì‹œë³´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì•ˆë‚´",
                success_criteria="ì ì ˆí•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë° ëŒ€ì•ˆ ì œì‹œ",
                category="error_handling"
            )
        ]
        
        return scenarios
    
    async def run_single_test(self, scenario: TestScenario) -> TestResult:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {scenario.name}")
        print(f"ğŸ“ ì„¤ëª…: {scenario.description}")
        print(f"ğŸ’¬ ì…ë ¥: {scenario.user_input}")
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Agentì— ìš”ì²­
            response_data = await process_chat_message(
                scenario.user_input, 
                f"test-{scenario.name}-{int(start_time)}"
            )
            
            execution_time = asyncio.get_event_loop().time() - start_time
            
            agent_response = response_data.get('content', '')
            agent_used = response_data.get('agent_used', 'unknown')
            tools_used = response_data.get('tools_used', [])
            
            print(f"ğŸ¤– ì‚¬ìš©ëœ ì—ì´ì „íŠ¸: {agent_used}")
            print(f"ğŸ› ï¸ ì‚¬ìš©ëœ ë„êµ¬: {tools_used}")
            print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(agent_response)} ë¬¸ì")
            print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
            
            # LLM í‰ê°€
            print("ğŸ” LLM í‰ê°€ ì¤‘...")
            llm_evaluation = await self.evaluator.evaluate_response(
                scenario, agent_response, agent_used, tools_used
            )
            
            success = llm_evaluation.get('success', False)
            print(f"âœ… í‰ê°€ ê²°ê³¼: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'} ({llm_evaluation.get('percentage', 0):.1f}%)")
            
            return TestResult(
                scenario=scenario,
                agent_response=agent_response,
                agent_used=agent_used,
                tools_used=tools_used,
                response_length=len(agent_response),
                execution_time=execution_time,
                llm_evaluation=llm_evaluation,
                success=success
            )
            
        except Exception as e:
            execution_time = asyncio.get_event_loop().time() - start_time
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            return TestResult(
                scenario=scenario,
                agent_response="",
                agent_used="error",
                tools_used=[],
                response_length=0,
                execution_time=execution_time,
                llm_evaluation={
                    "total_score": 0,
                    "max_score": 25,
                    "percentage": 0,
                    "success": False,
                    "summary": f"ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
                },
                success=False,
                error=str(e)
            )
    
    async def run_all_tests(self) -> List[TestResult]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        print("ğŸš€ ì¢…í•©ì ì¸ Agent í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"ğŸ“‹ ì´ {len(self.test_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰")
        print("=" * 60)
        
        results = []
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            print(f"\n[{i}/{len(self.test_scenarios)}]", end="")
            
            result = await self.run_single_test(scenario)
            results.append(result)
            
            # ê° í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
            await asyncio.sleep(2)
        
        self.results = results
        return results
    
    def generate_report(self) -> str:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        if not self.results:
            return "í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í†µê³„ ê³„ì‚°
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        success_rate = (successful_tests / total_tests) * 100
        
        avg_score = sum(r.llm_evaluation.get('percentage', 0) for r in self.results) / total_tests
        avg_execution_time = sum(r.execution_time for r in self.results) / total_tests
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        categories = {}
        for result in self.results:
            category = result.scenario.category
            if category not in categories:
                categories[category] = {'total': 0, 'success': 0, 'scores': []}
            
            categories[category]['total'] += 1
            if result.success:
                categories[category]['success'] += 1
            categories[category]['scores'].append(result.llm_evaluation.get('percentage', 0))
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = f"""
ğŸ“Š Agent í…ŒìŠ¤íŠ¸ ì¢…í•© ë¦¬í¬íŠ¸
{'=' * 50}

ğŸ“ˆ ì „ì²´ í†µê³„:
- ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ
- ì„±ê³µ: {successful_tests}ê°œ
- ì‹¤íŒ¨: {total_tests - successful_tests}ê°œ
- ì„±ê³µë¥ : {success_rate:.1f}%
- í‰ê·  ì ìˆ˜: {avg_score:.1f}%
- í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_execution_time:.2f}ì´ˆ

ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼:
"""
        
        for category, stats in categories.items():
            cat_success_rate = (stats['success'] / stats['total']) * 100
            cat_avg_score = sum(stats['scores']) / len(stats['scores'])
            report += f"  â€¢ {category}: {cat_success_rate:.1f}% ì„±ê³µ (í‰ê·  {cat_avg_score:.1f}ì )\n"
        
        report += f"\nğŸ” ìƒì„¸ ê²°ê³¼:\n"
        
        for result in self.results:
            status = "âœ… ì„±ê³µ" if result.success else "âŒ ì‹¤íŒ¨"
            score = result.llm_evaluation.get('percentage', 0)
            summary = result.llm_evaluation.get('summary', 'N/A')
            
            report += f"""
  ğŸ“‹ {result.scenario.name}
     - ìƒíƒœ: {status} ({score:.1f}%)
     - ì—ì´ì „íŠ¸: {result.agent_used}
     - ì‹¤í–‰ì‹œê°„: {result.execution_time:.2f}ì´ˆ
     - í‰ê°€: {summary}
"""
            
            if result.error:
                report += f"     - ì˜¤ë¥˜: {result.error}\n"
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        all_recommendations = []
        for result in self.results:
            recommendations = result.llm_evaluation.get('recommendations', [])
            all_recommendations.extend(recommendations)
        
        if all_recommendations:
            unique_recommendations = list(set(all_recommendations))
            report += f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:\n"
            for rec in unique_recommendations[:5]:  # ìƒìœ„ 5ê°œë§Œ
                report += f"  â€¢ {rec}\n"
        
        report += f"\nğŸ“… ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        return report
    
    def save_detailed_results(self, filename: str = None) -> str:
        """ìƒì„¸í•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"agent_test_results_{timestamp}.json"
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        results_data = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'total_tests': len(self.results),
                'successful_tests': sum(1 for r in self.results if r.success)
            },
            'results': []
        }
        
        for result in self.results:
            result_data = {
                'scenario': {
                    'name': result.scenario.name,
                    'description': result.scenario.description,
                    'user_input': result.scenario.user_input,
                    'expected_agent': result.scenario.expected_agent,
                    'category': result.scenario.category
                },
                'response': {
                    'content': result.agent_response,
                    'agent_used': result.agent_used,
                    'tools_used': result.tools_used,
                    'response_length': result.response_length,
                    'execution_time': result.execution_time
                },
                'evaluation': result.llm_evaluation,
                'success': result.success,
                'error': result.error
            }
            results_data['results'].append(result_data)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        return filename

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    tester = ComprehensiveAgentTester()
    
    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = await tester.run_all_tests()
    
    # ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
    print("\n" + "=" * 60)
    report = tester.generate_report()
    print(report)
    
    # ìƒì„¸ ê²°ê³¼ ì €ì¥
    filename = tester.save_detailed_results()
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main()) 