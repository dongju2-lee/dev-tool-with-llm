# Vertex AI Geminiì™€ LangGraph ì—°ë™ ê°€ì´ë“œ

## ê°œìš”

ì´ ë¬¸ì„œëŠ” Google Cloud Vertex AIì˜ Gemini ëª¨ë¸ê³¼ LangGraphë¥¼ Python ì½”ë“œì—ì„œ ì—°ë™í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. GeminiëŠ” Googleì˜ ìµœì‹  LLM(Large Language Model)ìœ¼ë¡œ, í…ìŠ¤íŠ¸ ìƒì„±, ì½”ë“œ ì‘ì„±, ë‹¤ì¤‘ ëª¨ë‹¬ ëŒ€í™” ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. LangGraphëŠ” ìƒíƒœ ê¸°ë°˜(stateful) LLM ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë˜í”„ í˜•íƒœë¡œ ì„¤ê³„í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Google Cloud ê³„ì • ë° í”„ë¡œì íŠ¸
- Vertex AI API í™œì„±í™”
- Python 3.7 ì´ìƒ
- ì ì ˆí•œ ê¶Œí•œ ì„¤ì • (Vertex AI ì‚¬ìš©ì ê¶Œí•œ)

## ì„¤ì¹˜ ë°©ë²•

```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install python-dotenv
pip install google-cloud-aiplatform
pip install -U langgraph
pip install streamlit
```

## API í‚¤ ê´€ë¦¬

Vertex AI API í‚¤ (ë˜ëŠ” ì„œë¹„ìŠ¤ ê³„ì • í‚¤)ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•:

```python
import os
from dotenv import load_dotenv
import vertexai
from vertexai.preview.generative_models import GenerativeModel

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸° 
VERTEX_API_KEY = os.getenv("VERTEX_API_KEY")  # API í‚¤ ì¸ì¦ ë°©ì‹
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
LOCATION = os.getenv("VERTEX_AI_LOCATION", "us-central1")

# Vertex AI ì´ˆê¸°í™” (API í‚¤ ë°©ì‹)
vertexai.init(project=PROJECT_ID, location=LOCATION, api_key=VERTEX_API_KEY)

# Gemini ëª¨ë¸ ë¡œë“œ
model = GenerativeModel("gemini-pro")
```

## ê¸°ë³¸ LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±

LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë˜í”„ë¡œ êµ¬ì„±í•˜ëŠ” ë°©ë²•:

```python
from langgraph.graph import MessageGraph, END

# Geminië¥¼ í˜¸ì¶œí•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def gemini_node(messages):
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
    user_input = messages[-1]["content"] if isinstance(messages[-1], dict) else messages[-1].content
    
    # Gemini ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
    response = model.generate_content(user_input)
    
    # ì‘ë‹µ ë°˜í™˜
    return [{"role": "assistant", "content": response.text}]

# ê°„ë‹¨í•œ LangGraph ë©”ì‹œì§€ ê·¸ë˜í”„ ìƒì„±
graph = MessageGraph()
graph.add_node("gemini", gemini_node)
graph.add_edge("gemini", END)
graph.set_entry_point("gemini")
runnable = graph.compile()
```

## Streamlit ì½”ë“œë¥¼ LangGraphë¡œ ë¦¬íŒ©í† ë§í•˜ê¸°

### ê¸°ì¡´ Streamlit ì•± êµ¬ì¡°

ê¸°ì¡´ Streamlit ì•±ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ íŒ¨í„´ì„ ê°€ì§‘ë‹ˆë‹¤:

```python
import streamlit as st
from api.client import MCPClient

# ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì‘ë‹µ ìƒì„±
    response = mcp_client.send_message(message=prompt, thread_id=thread_id)
    
    # ì„¸ì…˜ì— ì €ì¥
    st.session_state.messages.append({
        "role": "assistant", 
        "content": response_content
    })
```

### LangGraph ê¸°ë°˜ ë¦¬íŒ©í† ë§ 1: ê¸°ëŠ¥ë³„ ê·¸ë˜í”„ ë…¸ë“œ ë¶„ë¦¬

LangGraphë¥¼ ì‚¬ìš©í•œ ë¦¬íŒ©í† ë§ì˜ í•µì‹¬ì€ ê¸°ëŠ¥ì„ ê·¸ë˜í”„ ë…¸ë“œë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
import streamlit as st
import os
from dotenv import load_dotenv
import vertexai
from vertexai.preview.generative_models import GenerativeModel
from langgraph.graph import StateGraph
from typing import Dict, Any, TypedDict, List

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
VERTEX_API_KEY = os.getenv("VERTEX_API_KEY")
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
LOCATION = os.getenv("VERTEX_AI_LOCATION", "us-central1")

# ìƒíƒœ íƒ€ì… ì •ì˜
class ChatState(TypedDict):
    messages: List[Dict[str, str]]
    current_message: str
    response: str
    show_dashboard: bool

# ì´ˆê¸° ìƒíƒœ
def get_initial_state() -> ChatState:
    return {
        "messages": [],
        "current_message": "",
        "response": "",
        "show_dashboard": False
    }

# 1. ë©”ì‹œì§€ ì²˜ë¦¬ ë…¸ë“œ
def process_input(state: ChatState) -> ChatState:
    # í˜„ì¬ ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì¶”ê°€
    messages = state["messages"].copy()
    messages.append({"role": "user", "content": state["current_message"]})
    
    # ëŒ€ì‹œë³´ë“œ í‚¤ì›Œë“œ í™•ì¸
    show_dashboard = any(
        keyword in state["current_message"].lower() 
        for keyword in ["ëŒ€ì‹œë³´ë“œ", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ë³´ì—¬ì¤˜"]
    )
    
    return {"messages": messages, "show_dashboard": show_dashboard}

# 2. Gemini ì‘ë‹µ ìƒì„± ë…¸ë“œ 
def generate_response(state: ChatState) -> ChatState:
    # Vertex AI ì´ˆê¸°í™”
    vertexai.init(project=PROJECT_ID, location=LOCATION, api_key=VERTEX_API_KEY)
    model = GenerativeModel("gemini-pro")
    
    # ì „ì²´ ë©”ì‹œì§€ ì´ë ¥
    prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™”ì— ì‘ë‹µí•˜ì„¸ìš”:\n\n"
    for msg in state["messages"]:
        prompt += f"{msg['role']}: {msg['content']}\n"
    
    # Gemini í˜¸ì¶œ
    response = model.generate_content(prompt)
    
    # ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
    messages = state["messages"].copy()
    messages.append({"role": "assistant", "content": response.text})
    
    return {"messages": messages, "response": response.text}

# 3. ëŒ€ì‹œë³´ë“œ ìƒì„± ë…¸ë“œ (ì¡°ê±´ë¶€ ì‹¤í–‰)
def create_dashboard(state: ChatState) -> ChatState:
    if not state["show_dashboard"]:
        return {}  # ë³€ê²½ ì—†ìŒ
        
    # ëŒ€ì‹œë³´ë“œ ê´€ë ¨ ì‘ë‹µ ì¶”ê°€
    dashboard_response = "ì—¬ê¸° ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤! (ì´ë¯¸ì§€ëŠ” ì‹¤ì œ ë Œë”ë§ë  ë•Œ ìƒì„±ë©ë‹ˆë‹¤)"
    
    # ì‘ë‹µ ì—…ë°ì´íŠ¸
    messages = state["messages"].copy()
    if messages and messages[-1]["role"] == "assistant":
        # ê¸°ì¡´ ì‘ë‹µì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
        messages[-1]["content"] = dashboard_response
    else:
        # ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
        messages.append({"role": "assistant", "content": dashboard_response})
    
    return {"messages": messages, "response": dashboard_response}

# ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(ChatState)
workflow.add_node("process_input", process_input)
workflow.add_node("generate_response", generate_response)
workflow.add_node("create_dashboard", create_dashboard)

# ì—£ì§€ ë° ì¡°ê±´ë¶€ ì‹¤í–‰ ì •ì˜
workflow.add_edge("process_input", "create_dashboard")
workflow.add_conditional_edges(
    "create_dashboard",
    lambda state: "generate_response" if not state["show_dashboard"] else END
)
workflow.add_edge("generate_response", END)

# ì‹œì‘ì  ì„¤ì •
workflow.set_entry_point("process_input")

# ê·¸ë˜í”„ ì»´íŒŒì¼
chain = workflow.compile()
```

### LangGraph ê¸°ë°˜ ë¦¬íŒ©í† ë§ 2: Streamlitê³¼ í†µí•©

```python
import streamlit as st
import os
from dotenv import load_dotenv
from PIL import Image, ImageDraw
import io

# ìœ„ì—ì„œ ì •ì˜í•œ LangGraph ì»´í¬ë„ŒíŠ¸ ê°€ì ¸ì˜¤ê¸°
# from langchain_workflow import chain, get_initial_state

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LangGraph ìŠ¬ë¼ì„ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="centered"
)

# CSS ì¶”ê°€
st.markdown("""
<style>
    .stChatMessage {
        padding: 10px;
        border-radius: 10px;
        margin-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)

st.title("LangGraph ìŠ¬ë¼ì„ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "graph_state" not in st.session_state:
    st.session_state.graph_state = get_initial_state()

# ìŠ¬ë¼ì„ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ (ëŒ€ì‹œë³´ë“œ í‘œì‹œìš©)
def create_slime_image():
    img = Image.new('RGB', (400, 400), color=(255, 240, 200))
    draw = ImageDraw.Draw(img)
    
    # ìŠ¬ë¼ì„ ëª¸ì²´ (ì›)
    draw.ellipse((100, 120, 300, 320), fill=(255, 180, 80), outline=(101, 67, 33), width=3)
    
    # ëˆˆ
    draw.ellipse((150, 180, 190, 220), fill=(0, 0, 0), outline=(0, 0, 0))
    draw.ellipse((210, 180, 250, 220), fill=(0, 0, 0), outline=(0, 0, 0))
    
    # ëˆˆ í•˜ì´ë¼ì´íŠ¸
    draw.ellipse((155, 185, 170, 200), fill=(255, 255, 255), outline=(255, 255, 255))
    draw.ellipse((215, 185, 230, 200), fill=(255, 255, 255), outline=(255, 255, 255))
    
    # ì…
    draw.arc((175, 220, 225, 270), start=0, end=180, fill=(101, 67, 33), width=3)
    
    # ë³¼
    draw.ellipse((140, 220, 160, 240), fill=(255, 150, 150), outline=(255, 150, 150))
    draw.ellipse((240, 220, 260, 240), fill=(255, 150, 150), outline=(255, 150, 150))
    
    # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë°˜í™˜
    img_bytes = io.BytesIO()
    img.save(img_bytes, format="PNG")
    img_bytes.seek(0)
    
    return img_bytes

# ëŒ€í™” ì´ë ¥ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message.get("type") == "image":
            st.markdown(message.get("text", ""))
            st.image(message["content"], caption=message.get("caption", ""), use_column_width=True)
        else:
            st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤€ë¹„
    current_state = st.session_state.graph_state.copy()
    current_state["current_message"] = prompt
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            # LangGraph ì²´ì¸ ì‹¤í–‰
            result = chain.invoke(current_state)
            
            # ê²°ê³¼ ì²˜ë¦¬ ë° í‘œì‹œ
            if result["show_dashboard"]:
                # ëŒ€ì‹œë³´ë“œ ì´ë¯¸ì§€ ìƒì„± ë° í‘œì‹œ
                image = create_slime_image()
                response_text = "ì—¬ê¸° ìŠ¬ë¼ì„ ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤!"
                st.markdown(response_text)
                st.image(image, caption="ìŠ¬ë¼ì„ ëŒ€ì‹œë³´ë“œ", use_column_width=True)
                
                # ì´ë¯¸ì§€ ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant",
                    "type": "image",
                    "content": image,
                    "caption": "ìŠ¬ë¼ì„ ëŒ€ì‹œë³´ë“œ",
                    "text": response_text
                })
            else:
                # í…ìŠ¤íŠ¸ ì‘ë‹µ í‘œì‹œ
                st.markdown(result["response"])
                
                # í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": result["response"]
                })
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state.graph_state = result
```

### LangGraph ê¸°ëŠ¥ë³„ ì„¤ê³„ ê°€ì´ë“œ

LangGraphë¡œ ë¦¬íŒ©í† ë§í•  ë•Œ ì£¼ìš” ê³ ë ¤ì‚¬í•­:

1. **ìƒíƒœ íƒ€ì… ì •ì˜**:
   - TypedDictë¡œ ê·¸ë˜í”„ê°€ ê´€ë¦¬í•˜ëŠ” ìƒíƒœì˜ íƒ€ì…ì„ ëª…í™•íˆ ì •ì˜
   - ë©”ì‹œì§€ ê¸°ë¡, í˜„ì¬ ì…ë ¥, ì‘ë‹µ, í”Œë˜ê·¸ ë“±ì„ í¬í•¨

2. **ê¸°ëŠ¥ë³„ ë…¸ë“œ ë¶„ë¦¬**:
   - ê° ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë…ë¦½ì ì¸ ë…¸ë“œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
   - ì…ë ¥ ì²˜ë¦¬, ì‘ë‹µ ìƒì„±, íŠ¹ìˆ˜ ì‘ë‹µ(ëŒ€ì‹œë³´ë“œ ë“±) ìƒì„±ì„ ë³„ë„ ë…¸ë“œë¡œ êµ¬ì„±

3. **ì¡°ê±´ë¶€ íë¦„ ì œì–´**:
   - `add_conditional_edges`ë¡œ ìƒíƒœì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ êµ¬í˜„
   - í‚¤ì›Œë“œ ê°ì§€, ì‘ë‹µ ìœ í˜• ë“±ì— ë”°ë¼ ë‹¤ë¥¸ ê²½ë¡œë¡œ ì²˜ë¦¬

4. **ê²°ê³¼ í†µí•© ë° ìƒíƒœ ê´€ë¦¬**:
   - Streamlit ì„¸ì…˜ ìƒíƒœì™€ LangGraph ìƒíƒœë¥¼ ë™ê¸°í™”
   - ì›Œí¬í”Œë¡œìš° ê²°ê³¼ë¥¼ UIì— ì ì ˆíˆ ë°˜ì˜

5. **ì¸ê°„ ê°œì… í™œìš©** (í™•ì¥ ê°€ëŠ¥):
   ```python
   # ì¸ê°„ ê°œì…ì´ í•„ìš”í•œ ì¡°ê±´ ì •ì˜
   def should_ask_human(state):
       return "ë¶ˆí™•ì‹¤" in state["response"] or state["show_dashboard"]
       
   # ì¸ê°„ ê°œì… ë…¸ë“œ
   def human_review(state):
       # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì¼ì‹œì¤‘ì§€í•˜ê³  ì‚¬ìš©ì ì…ë ¥ ê¸°ë‹¤ë¦¼
       return {"human_approved": True}  # Streamlitì—ì„œ ë²„íŠ¼ í´ë¦­ ë“±ìœ¼ë¡œ ì„¤ì •
       
   # ì¡°ê±´ë¶€ ì—ì§€ ì¶”ê°€
   workflow.add_conditional_edges(
       "generate_response",
       lambda state: "human_review" if should_ask_human(state) else "next_node"
   )
   ```

## ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ: í†µí•© ìŠ¬ë¼ì„ ì±—ë´‡

ìœ„ ê°œë…ì„ í†µí•©í•œ ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ:

```python
# langgraph_bot.py - ë©”ì¸ LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
import os
from dotenv import load_dotenv
import vertexai
from vertexai.preview.generative_models import GenerativeModel
from typing import Dict, List, Any, TypedDict
from langgraph.graph import StateGraph, END

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
VERTEX_API_KEY = os.getenv("VERTEX_API_KEY")  # API í‚¤ ë°©ì‹ ì‚¬ìš©
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
LOCATION = os.getenv("VERTEX_AI_LOCATION", "us-central1")

# ìƒíƒœ íƒ€ì… ì •ì˜
class ChatState(TypedDict):
    messages: List[Dict[str, Any]]  # ì±„íŒ… ì´ë ¥
    current_input: str              # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
    response: str                   # í˜„ì¬ ì‘ë‹µ
    dashboard_mode: bool            # ëŒ€ì‹œë³´ë“œ ëª¨ë“œ ì—¬ë¶€
    error: str                      # ì˜¤ë¥˜ ë©”ì‹œì§€

# ì´ˆê¸° ìƒíƒœ ìƒì„± í•¨ìˆ˜
def create_initial_state() -> ChatState:
    return {
        "messages": [],
        "current_input": "",
        "response": "",
        "dashboard_mode": False,
        "error": ""
    }

# ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì •ì˜
def process_input(state: ChatState) -> Dict:
    """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ëª¨ë“œ ê²°ì •"""
    try:
        # í˜„ì¬ ì…ë ¥ì—ì„œ ëŒ€ì‹œë³´ë“œ í‚¤ì›Œë“œ ê°ì§€
        is_dashboard_request = any(
            keyword in state["current_input"].lower() 
            for keyword in ["ëŒ€ì‹œë³´ë“œ", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ë³´ì—¬ì¤˜"]
        )
        
        # ë©”ì‹œì§€ ì´ë ¥ ì—…ë°ì´íŠ¸
        messages = state["messages"].copy()
        messages.append({"role": "user", "content": state["current_input"]})
        
        return {
            "messages": messages,
            "dashboard_mode": is_dashboard_request
        }
    except Exception as e:
        return {"error": f"ì…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"}

def generate_llm_response(state: ChatState) -> Dict:
    """Gemini ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
    try:
        # Vertex AI ì´ˆê¸°í™”
        vertexai.init(project=PROJECT_ID, location=LOCATION, api_key=VERTEX_API_KEY)
        model = GenerativeModel("gemini-pro")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìŠ¬ë¼ì„ ìºë¦­í„° AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™”ì— ì‘ë‹µí•˜ì„¸ìš”:\n\n"
        for msg in state["messages"]:
            if msg.get("role") == "user":
                context += f"ì‚¬ìš©ì: {msg['content']}\n"
            elif msg.get("role") == "assistant":
                context += f"ìŠ¬ë¼ì„: {msg['content']}\n"
        
        # ëª¨ë¸ í˜¸ì¶œ
        response = model.generate_content(context)
        
        # ì‘ë‹µ ì €ì¥
        return {"response": response.text}
    except Exception as e:
        return {"error": f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"}

def update_messages(state: ChatState) -> Dict:
    """ë©”ì‹œì§€ ì´ë ¥ ì—…ë°ì´íŠ¸"""
    messages = state["messages"].copy()
    messages.append({"role": "assistant", "content": state["response"]})
    return {"messages": messages}

def create_dashboard_response(state: ChatState) -> Dict:
    """ëŒ€ì‹œë³´ë“œ ì‘ë‹µ ìƒì„±"""
    response = "ìŠ¬ë¼ì„ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í–ˆì–´ìš”! (ëŒ€ì‹œë³´ë“œëŠ” Streamlit UIì—ì„œ ë Œë”ë§ë©ë‹ˆë‹¤)"
    messages = state["messages"].copy()
    messages.append({"role": "assistant", "content": response, "is_dashboard": True})
    return {"messages": messages, "response": response}

# ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±
def create_chat_workflow() -> StateGraph:
    workflow = StateGraph(ChatState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("process_input", process_input)
    workflow.add_node("generate_llm_response", generate_llm_response)
    workflow.add_node("update_messages", update_messages)
    workflow.add_node("create_dashboard", create_dashboard_response)
    
    # ì—ì§€ ë° ì¡°ê±´ë¶€ ë¶„ê¸° ì¶”ê°€
    workflow.add_edge("process_input", lambda state: 
        "create_dashboard" if state["dashboard_mode"] else "generate_llm_response")
    workflow.add_edge("generate_llm_response", "update_messages")
    workflow.add_edge("update_messages", END)
    workflow.add_edge("create_dashboard", END)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("process_input")
    
    return workflow

# ì›Œí¬í”Œë¡œìš° ìƒì„± ë° ì»´íŒŒì¼
chat_workflow = create_chat_workflow()
chat_chain = chat_workflow.compile()
```

```python
# app.py - Streamlit ì¸í„°í˜ì´ìŠ¤
import streamlit as st
from PIL import Image, ImageDraw
import io
from langgraph_bot import chat_chain, create_initial_state

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LangGraph ìŠ¬ë¼ì„ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="centered"
)

# íƒ€ì´í‹€ ì„¤ì •
st.title("LangGraph ìŠ¬ë¼ì„ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ìŠ¬ë¼ì„ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜
def create_slime_image():
    # (ì´ì „ê³¼ ë™ì¼í•œ ì´ë¯¸ì§€ ìƒì„± ì½”ë“œ)
    # ...
    return img_bytes

# ëŒ€í™” ì´ë ¥ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message.get("is_dashboard", False):
            # ëŒ€ì‹œë³´ë“œ ë©”ì‹œì§€ì˜ ê²½ìš° ì´ë¯¸ì§€ë„ í‘œì‹œ
            st.markdown(message["content"])
            # ì´ë¯¸ì§€ ìƒì„± ë° í‘œì‹œ
            image = create_slime_image()
            st.image(image, caption="ìŠ¬ë¼ì„ ëŒ€ì‹œë³´ë“œ", use_column_width=True)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€
            st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # LangGraph ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì´ˆê¸°í™”
    state = create_initial_state()
    state["messages"] = st.session_state.messages[:-1]  # í˜„ì¬ ì…ë ¥ ì œì™¸ ì´ì „ ì´ë ¥
    state["current_input"] = prompt
    
    # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë° ê²°ê³¼ íšë“
            result = chat_chain.invoke(state)
            
            # ê²°ê³¼ í‘œì‹œ
            last_message = result["messages"][-1]
            if last_message.get("is_dashboard", False):
                # ëŒ€ì‹œë³´ë“œ ë©”ì‹œì§€ì˜ ê²½ìš° ì´ë¯¸ì§€ë„ í‘œì‹œ
                st.markdown(last_message["content"])
                image = create_slime_image()
                st.image(image, caption="ìŠ¬ë¼ì„ ëŒ€ì‹œë³´ë“œ", use_column_width=True)
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€
                st.markdown(last_message["content"])
                
            # ê²°ê³¼ ì €ì¥
            st.session_state.messages = result["messages"]
```

## ì¶”ê°€ ê¸°ëŠ¥ í™•ì¥

1. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: LangGraphì˜ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ëŒ€í™” ì´ë ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   ```python
   # ë¶„í•  ì •ë³µ íŒ¨í„´
   from langgraph.graph import END, StateGraph
   from langgraph.pregel import Pregel
   
   # ë³‘ë ¬ ì²˜ë¦¬í•  ë…¸ë“œë“¤ ì •ì˜
   # ...
   
   # Pregel í™œìš© ë³‘ë ¬ ì²˜ë¦¬
   pregel = Pregel(lambda: {"result": ""})
   pregel.map("parallel_task", parallel_task_function)
   pregel.combine("combine_results", combine_function)
   
   # ë©”ì¸ ê·¸ë˜í”„ì— í†µí•©
   workflow.add_node("parallel_processing", pregel)
   ```

3. **ì¸ê°„ ê°œì… (Human-in-the-loop)**: í•„ìš”ì‹œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¼ì‹œ ì¤‘ì§€í•˜ê³  ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   ```python
   from langgraph.checkpoint.memory import MemorySaver
   
   # ì²´í¬í¬ì¸í„° ì„¤ì •
   checkpointer = MemorySaver()
   
   # ì¸ê°„ ê°œì… ë…¸ë“œ ì •ì˜
   def human_intervention(state):
       # ì´ í•¨ìˆ˜ëŠ” Streamlit ì•±ì—ì„œ ë‹¤ì‹œ í˜¸ì¶œë  ë•Œê¹Œì§€ ëŒ€ê¸°
       return {}
   
   # ê·¸ë˜í”„ êµ¬ì„± ì‹œ ì²´í¬í¬ì¸í„° ì„¤ì •
   workflow = StateGraph(ChatState, checkpointer=checkpointer)
   
   # ì¸ê°„ ê°œì… ë…¸ë“œ ì¶”ê°€
   workflow.add_node("human_review", human_intervention)
   ```

## ë¬¸ì œ í•´ê²°

1. **API í‚¤ ì˜¤ë¥˜**: API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

2. **ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì˜¤ë¥˜**: ê° ë…¸ë“œì˜ ì…ì¶œë ¥ì´ íƒ€ì… ì‹œìŠ¤í…œê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

3. **ì„±ëŠ¥ ìµœì í™”**: ëŒ€ìš©ëŸ‰ ìƒíƒœëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í•„ìš”í•œ ë°ì´í„°ë§Œ ìƒíƒœì— ìœ ì§€í•˜ì„¸ìš”.

## ì¶”ê°€ ìë£Œ

- [Vertex AI ê³µì‹ ë¬¸ì„œ](https://cloud.google.com/vertex-ai/docs)
- [LangGraph ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/docs/langgraph)
- [Gemini API ë ˆí¼ëŸ°ìŠ¤](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini)

## íŒê³¼ ëª¨ë²” ì‚¬ë¡€

1. **í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬**
   - ì¤‘ìš”í•œ ì¸ì¦ ì •ë³´ì™€ ì„¤ì •ì€ í•­ìƒ `.env` íŒŒì¼ì— ë³´ê´€í•˜ê³  ë²„ì „ ì œì–´ì—ì„œ ì œì™¸í•˜ì„¸ìš”.
   - `python-dotenv` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. **ê·¸ë˜í”„ ì„¤ê³„**
   - ë‹¨ìˆœí•œ ì›Œí¬í”Œë¡œìš°ëŠ” `MessageGraph`ë¥¼, ë³µì¡í•œ ìƒíƒœ ê´€ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° `StateGraph`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
   - ë…¸ë“œ í•¨ìˆ˜ëŠ” ëª…í™•í•˜ê³  ë‹¨ì¼ ì±…ì„ì„ ê°€ì§€ë„ë¡ ì„¤ê³„í•˜ì„¸ìš”.

3. **ë¹„ìš© ë° ì„±ëŠ¥ ìµœì í™”**
   - ê°œë°œ ì¤‘ì—ëŠ” `gemini-pro`ë³´ë‹¤ ë¹ ë¥´ê³  ì €ë ´í•œ `gemini-flash`ì™€ ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³´ì„¸ìš”.
   - ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë„êµ¬ë¥¼ ì„¤ì •í•˜ì—¬ API ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ì„¸ìš”.
