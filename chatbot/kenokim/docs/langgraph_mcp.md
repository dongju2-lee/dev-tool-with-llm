# LangGraphì™€ MCP(Model Context Protocol) ì—°ë™ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” LangGraphì™€ MCP(Model Context Protocol)ë¥¼ ì—°ë™í•˜ëŠ” ë°©ë²•ì„ Python ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

ë‹¤ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤:

```bash
pip install mcp==0.1.6                   # MCP ì½”ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ (2025ë…„ ìµœì‹  ë²„ì „: 1.6.0)
pip install langchain-core>=0.1.28       # LangChain ì½”ì–´
pip install langgraph>=0.2.0             # LangGraph (2025ë…„ ìµœì‹  ë²„ì „: 0.4.1)
pip install langchain-mcp-adapters>=0.0.5 # LangChain MCP ì–´ëŒ‘í„° (2025ë…„ ìµœì‹  ë²„ì „: 0.0.9)
```

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶œì²˜ ë° ì—…ë°ì´íŠ¸ ìƒíƒœ

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì¶œì²˜ | 2025ë…„ ìƒíƒœ | ìµœì‹  ë²„ì „ |
|------------|------|-------------|------------|
| mcp | [GitHub - modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk) | í™œë°œíˆ ìœ ì§€ë³´ìˆ˜ ì¤‘ | 1.6.0 (2025ë…„ 3ì›” 27ì¼) |
| langgraph | [LangChain - LangGraph](https://langchain-ai.github.io/langgraph/) | í™œë°œíˆ ìœ ì§€ë³´ìˆ˜ ì¤‘ | 0.4.1 (2025ë…„ 4ì›” 30ì¼) |
| langchain-mcp-adapters | [GitHub - langchain-ai/langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters) | í™œë°œíˆ ìœ ì§€ë³´ìˆ˜ ì¤‘ | 0.0.9 (2025ë…„ 4ì›” 16ì¼) |

LLM ì œê³µìì— ë”°ë¼ ì¶”ê°€ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
# OpenAIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
pip install langchain-openai

# Vertex AI (Gemini)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
pip install langchain-google-vertexai
pip install google-cloud-aiplatform
```

## 2. MCP ì„œë²„ êµ¬í˜„ (Python)

### 2.1 ê¸°ë³¸ MCP ì„œë²„ êµ¬ì¡°

MCP ì„œë²„ëŠ” FastMCPë¥¼ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
"""
ê°„ë‹¨í•œ MCP ì„œë²„ ì˜ˆì œ

ì´ ëª¨ë“ˆì€ FastMCPë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ìˆ˜í•™ ì—°ì‚° ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” MCP ì„œë²„ì…ë‹ˆë‹¤.
"""

from mcp.server.fastmcp import FastMCP
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastMCP ì„œë²„ ìƒì„±
mcp = FastMCP("MathTools")

@mcp.tool()
def add(a: int, b: int) -> int:
    """ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤.
    
    Args:
        a: ì²« ë²ˆì§¸ ìˆ«ì
        b: ë‘ ë²ˆì§¸ ìˆ«ì
        
    Returns:
        ë‘ ìˆ«ìì˜ í•©
    """
    logger.info(f"add ë„êµ¬ í˜¸ì¶œ: {a} + {b}")
    return a + b

# ë” ë§ì€ ë„êµ¬ ì¶”ê°€...

if __name__ == "__main__":
    # stdio ëª¨ë“œë¡œ ì„œë²„ ì‹¤í–‰
    try:
        logger.info("MCP ì„œë²„ ì‹œì‘...")
        mcp.run(transport="stdio")
    except KeyboardInterrupt:
        logger.info("ì„œë²„ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")
```

### 2.2 ë„êµ¬ ë°ì½”ë ˆì´í„° í™œìš©

MCP ì„œë²„ì˜ ë„êµ¬ëŠ” `@mcp.tool()` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•©ë‹ˆë‹¤:

```python
@mcp.tool()
def multiply(a: int, b: int) -> int:
    """ë‘ ìˆ«ìë¥¼ ê³±í•©ë‹ˆë‹¤.
    
    Args:
        a: ì²« ë²ˆì§¸ ìˆ«ì
        b: ë‘ ë²ˆì§¸ ìˆ«ì
        
    Returns:
        ë‘ ìˆ«ìì˜ ê³±
    """
    logger.info(f"multiply ë„êµ¬ í˜¸ì¶œ: {a} * {b}")
    return a * b

@mcp.tool()
def divide(a: int, b: int) -> float:
    """ì²« ë²ˆì§¸ ìˆ«ìë¥¼ ë‘ ë²ˆì§¸ ìˆ«ìë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    
    Args:
        a: ì²« ë²ˆì§¸ ìˆ«ì (ë‚˜ëˆ„ì–´ì§ˆ ìˆ˜)
        b: ë‘ ë²ˆì§¸ ìˆ«ì (ë‚˜ëˆ„ëŠ” ìˆ˜)
        
    Returns:
        ë‚˜ëˆ—ì…ˆ ê²°ê³¼
    """
    if b == 0:
        raise ValueError("0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    logger.info(f"divide ë„êµ¬ í˜¸ì¶œ: {a} / {b}")
    return a / b
```

### 2.3 MCP ì„œë²„ ì‹¤í–‰ ë°©ë²•

MCP ì„œë²„ëŠ” stdio í†µì‹ ì„ í†µí•´ í´ë¼ì´ì–¸íŠ¸ì™€ í†µì‹ í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
python math_server.py
```

ì¼ë°˜ì ìœ¼ë¡œ MCP ì„œë²„ëŠ” ì§ì ‘ ì‹¤í–‰í•˜ì§€ ì•Šê³ , MCP í´ë¼ì´ì–¸íŠ¸ì— ì˜í•´ subprocessë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

## 3. LangGraphì™€ MCP ì—°ë™

### 3.1 LangGraphìš© MCP í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„

LangGraphì™€ MCPë¥¼ ì—°ë™í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤:

```python
import os
import logging
import asyncio
from typing import List, Dict, Any, Tuple, Optional
import uuid

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_google_vertexai import ChatVertexAI  # ë˜ëŠ” ë‹¤ë¥¸ LLM ì œê³µì
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint import InMemoryCheckpointer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MCPLangGraphClient:
    """LangGraphì™€ MCPë¥¼ ì—°ë™í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, 
                 server_script_path: str,
                 model_name: str = "gemini-1.5-flash"):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            server_script_path: MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„
        """
        self.server_script_path = server_script_path
        self.model_name = model_name
        
        # í´ë¼ì´ì–¸íŠ¸ ì»´í¬ë„ŒíŠ¸
        self.session = None
        self.agent = None
        self.tools = []
        
        # ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    async def _initialize_session(self) -> Tuple[ClientSession, List[Any]]:
        """MCP ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ê³  ë„êµ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        logger.info(f"MCP ì„œë²„ ì—°ê²° ì¤‘: {self.server_script_path}")
        
        server_params = StdioServerParameters(
            command="python",
            args=[self.server_script_path],
        )
        
        # stdio í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        read, write = await stdio_client(server_params)
        session = await ClientSession(read, write)
        await session.initialize()
        
        # MCP ë„êµ¬ ë¡œë“œ
        tools = await load_mcp_tools(session)
        logger.info(f"ë„êµ¬ ë¡œë“œ ì™„ë£Œ: {len(tools)}ê°œ")
        
        return session, tools
    
    async def _setup_agent(self) -> Any:
        """ReAct ì—ì´ì „íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        # ì„¸ì…˜ ë° ë„êµ¬ ì´ˆê¸°í™”
        self.session, self.tools = await self._initialize_session()
        
        # LLM ì„¤ì •
        llm = ChatVertexAI(
            model_name=self.model_name,
            max_output_tokens=1024,
            temperature=0.1,
        )
        
        # ReAct ì—ì´ì „íŠ¸ ìƒì„±
        agent = create_react_agent(
            llm=llm,
            tools=self.tools,
            system_message="ë‹¹ì‹ ì€ MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."
        )
        
        # ì²´í¬í¬ì¸í„° ì„¤ì •
        checkpointer = InMemoryCheckpointer()
        
        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ì¶”ê°€
        return agent.with_config({"checkpointer": checkpointer})
    
    def initialize(self):
        """í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.agent = self.loop.run_until_complete(self._setup_agent())
        logger.info("ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def close(self):
        """ì„¸ì…˜ì„ ë‹«ìŠµë‹ˆë‹¤."""
        if self.session:
            self.loop.run_until_complete(self.session.close())
            self.session = None
            logger.info("ì„¸ì…˜ ì¢…ë£Œë¨")
    
    def process_query(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        async def _process():
            if not self.agent:
                logger.warning("ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                self.agent = await self._setup_agent()
            
            # ì¿¼ë¦¬ ì²˜ë¦¬
            messages = [HumanMessage(content=query)]
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            response_messages = []
            async for event in self.agent.astream({"messages": messages}):
                if "messages" in event:
                    response_messages = event["messages"]
            
            # ê²°ê³¼ ì¶”ì¶œ
            final_response = ""
            tool_outputs = []
            
            for msg in response_messages:
                if isinstance(msg, AIMessage) and msg not in messages:
                    final_response = msg.content
                elif isinstance(msg, ToolMessage) and msg not in messages:
                    tool_outputs.append({"tool": msg.name, "result": msg.content})
            
            return {
                "response": final_response,
                "tool_outputs": tool_outputs
            }
        
        return self.loop.run_until_complete(_process())
```

### 3.2 LangGraphì™€ MCP ì—°ë™ ì˜ˆì œ

ì•„ë˜ëŠ” MCP ì„œë²„ë¥¼ LangGraph ReAct ì—ì´ì „íŠ¸ì™€ ì—°ë™í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤:

```python
# ì„œë²„ ê²½ë¡œ ì„¤ì •
server_script_path = "math_server.py"

# í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì´ˆê¸°í™”
client = MCPLangGraphClient(server_script_path, model_name="gemini-1.5-flash")
client.initialize()

try:
    # ì¿¼ë¦¬ ì²˜ë¦¬
    result = client.process_query("15ì™€ 27ì„ ë”í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?")
    
    # ì‘ë‹µ ì¶œë ¥
    print(f"ì‘ë‹µ: {result['response']}")
    
    # ë„êµ¬ ì¶œë ¥ í‘œì‹œ
    for tool_output in result.get("tool_outputs", []):
        print(f"ë„êµ¬: {tool_output.get('tool', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
        print(f"ê²°ê³¼: {tool_output.get('result', '')}")
finally:
    # ì„¸ì…˜ ì¢…ë£Œ
    client.close()
```

## 4. MCPì™€ LangGraph ReAct ì—ì´ì „íŠ¸ í†µí•©

### 4.1 ReAct ì—ì´ì „íŠ¸ íŒ¨í„´

LangGraphì˜ ReAct ì—ì´ì „íŠ¸ íŒ¨í„´ì€ ë‹¤ìŒê³¼ ê°™ì€ íë¦„ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤:

1. ì‚¬ìš©ìê°€ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.
2. LLMì´ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ë„êµ¬ ì‹¤í–‰ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
3. ë„êµ¬ê°€ í•„ìš”í•œ ê²½ìš°, í•´ë‹¹ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
4. ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
5. LLMì´ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

### 4.2 ì£¼ìš” êµ¬ì„± ìš”ì†Œ

MCPì™€ LangGraph í†µí•©ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ:

1. **ClientSession**: MCP ì„œë²„ì™€ì˜ í†µì‹ ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
2. **load_mcp_tools**: MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ LangChain ë„êµ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
3. **create_react_agent**: ReAct ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
4. **InMemoryCheckpointer**: ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

### 4.3 ë°ì´í„° íë¦„

MCPì™€ LangGraph í†µí•©ì˜ ë°ì´í„° íë¦„:

```
ì‚¬ìš©ì ì¿¼ë¦¬ -> LangGraph ì—ì´ì „íŠ¸ -> LLM ì¶”ë¡  -> MCP ë„êµ¬ í˜¸ì¶œ -> 
ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ -> LLM ì¶”ë¡  -> ìµœì¢… ì‘ë‹µ -> ì‚¬ìš©ì
```

## 5. ì‹¤ì „ ì˜ˆì œ: Streamlit ì•±ì—ì„œ LangGraph MCP í™œìš©

Streamlit ì•±ì—ì„œ LangGraph MCPë¥¼ í™œìš©í•˜ëŠ” ì˜ˆì œ:

```python
import streamlit as st
import os
from mcp_langgraph_client import MCPLangGraphClient

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LangGraph MCP ë°ëª¨",
    page_icon="ğŸ¤–",
    layout="centered"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "client" not in st.session_state:
    st.session_state.client = None

# ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
server_script_path = os.path.join(current_dir, "math_server.py")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    
    # ëª¨ë¸ ì„ íƒ
    model_name = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro"]
    )
    
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"):
        with st.spinner("MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘..."):
            try:
                # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
                if st.session_state.client:
                    st.session_state.client.close()
                
                # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì´ˆê¸°í™”
                st.session_state.client = MCPLangGraphClient(
                    server_script_path=server_script_path,
                    model_name=model_name
                )
                st.session_state.client.initialize()
                st.success("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
                st.session_state.client = None

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
st.title("LangGraph MCP ë°ëª¨")

# ë°ëª¨ ì„¤ëª…
st.markdown("""
ì´ ë°ëª¨ëŠ” LangGraphì™€ MCPë¥¼ ì—°ë™í•˜ì—¬ ìˆ˜í•™ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.
ì‚¬ì´ë“œë°”ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•œ í›„, ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ë³´ì„¸ìš”.

ì˜ˆì‹œ ì§ˆë¬¸:
- "15ì™€ 27ì„ ë”í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?"
- "42ì—ì„œ 17ì„ ë¹¼ë©´ ì–¼ë§ˆê°€ ë˜ë‚˜ìš”?"
- "8ê³¼ 9ë¥¼ ê³±í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?"
""")

# ì‚¬ìš©ì ì…ë ¥
query = st.text_input("ì§ˆë¬¸ ì…ë ¥:")

if query and st.session_state.client:
    with st.spinner("ì²˜ë¦¬ ì¤‘..."):
        try:
            # ì¿¼ë¦¬ ì²˜ë¦¬
            result = st.session_state.client.process_query(query)
            
            # ì‘ë‹µ í‘œì‹œ
            st.markdown("### ì‘ë‹µ")
            st.write(result["response"])
            
            # ë„êµ¬ ì •ë³´ í‘œì‹œ
            st.markdown("### ì‚¬ìš©ëœ ë„êµ¬")
            for tool_output in result.get("tool_outputs", []):
                st.info(f"ë„êµ¬: {tool_output.get('tool', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                st.code(tool_output.get("result", ""))
        except Exception as e:
            st.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
elif query:
    st.warning("í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
```

## 6. ì¶”ê°€ íŒê³¼ ëª¨ë²” ì‚¬ë¡€

### 6.1 í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬

MCP ì„œë²„ì™€ LLM API í‚¤ì™€ ê°™ì€ ë¯¼ê°í•œ ì •ë³´ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤:

```python
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
api_key = os.getenv("VERTEX_API_KEY")
project_id = os.getenv("GCP_PROJECT_ID")
```

### 6.2 ì˜ˆì™¸ ì²˜ë¦¬

MCP ì„œë²„ì™€ì˜ í†µì‹ ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜ˆì™¸ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤:

```python
try:
    # MCP ë„êµ¬ í˜¸ì¶œ
    result = await self.session.call_tool(name=tool_name, arguments=tool_args)
    return result
except Exception as e:
    logger.error(f"ë„êµ¬ í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
    return {"error": str(e)}
```

### 6.3 ë¡œê¹…

ë””ë²„ê¹…ì„ ìœ„í•´ ìì„¸í•œ ë¡œê¹…ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤:

```python
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
```

### 6.4 ë¹„ë™ê¸° ì²˜ë¦¬

MCP í†µì‹ ì€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤:

```python
async def process_query_async(self, query: str):
    # ë¹„ë™ê¸° ì²˜ë¦¬ ì½”ë“œ
    pass

def process_query(self, query: str):
    # ë™ê¸° ë˜í¼
    return self.loop.run_until_complete(self.process_query_async(query))
```

## ìš”ì•½

LangGraphì™€ MCPë¥¼ ì—°ë™í•˜ë©´ ë‹¤ì–‘í•œ ì™¸ë¶€ ë„êµ¬ë¥¼ LLM ì—ì´ì „íŠ¸ì— ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ë‹¨ê³„ëŠ”:

1. MCP ì„œë²„ êµ¬í˜„: FastMCPë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ë¥¼ ì •ì˜í•˜ê³  stdio ëª¨ë“œë¡œ ì‹¤í–‰
2. LangGraph í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„: MCP ë„êµ¬ë¥¼ ë¡œë“œí•˜ê³  ReAct ì—ì´ì „íŠ¸ì™€ í†µí•©
3. ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬: ì—ì´ì „íŠ¸ê°€ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ í˜¸ì¶œ
4. ê²°ê³¼ ë°˜í™˜: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ì™€ í•¨ê»˜ ìµœì¢… ì‘ë‹µ ì œê³µ

ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ê°•ë ¥í•˜ê³  ìœ ì—°í•œ AI ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
